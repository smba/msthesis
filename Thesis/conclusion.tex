In this chapter, we finish this thesis by providing a brief conclusion, a reviewof limitations encountered, and an outlook on possible future research directions.\section{Concluding Remarks}In this thesis, we have presented a methodology to assess the performanceevolution history of configurable software systems. Our methodology provides acomprehensive and informed means to analyze performance of software systemswith respect to the dimensions of variability and evolution.\paragraph{Methodology summary.} In the first part of our methodology,chapter\,\ref{sec:chapter:3}, we have provide the user a guideline on how to synthesize avariability model describing valid selections of configuration options. The guideline is driven by the extent of which variability is documented for the software system; for three differentscenarios (cf. Table\,\ref{tab:synthesis}), we advocate the use of existingvariability models, family-based analysis approaches, or a bottom-up strategy to pursue based ondocumentation assets (cf. Table\,\ref{tab:manual_var_assessment}). The second part of our methodology in chapter\,\ref{chapter:4}, hasaddressed the question of how to select a subset of versions of a configurablesoftware system to sketch performance evolution accurately and efficiently. Wehave proposed and evaluated five different strategies based on assumedrelationships between software metrics and the impact of a revision on thesoftware system’s performance. The findings of our evaluation suggest thatversions, for which large code sections are revised, are a good foundation forsuch version sample sets. Moreover, we were able to learn from larger revisionsthe possible impact of modifying a file on the overall software systemperformance. We have successfully tested the sampling strategies on a matureconfigurable software system with a development history of about ten years.Finally, in the third part of our methodology in chapter\,\ref{chapter:5}, wehave presented the practical aspects of performance measurement, including the selection ofsuitable performance benchmarks, the choice of profiling tools, and statisticalmeans to summarize measurement results. In addition, we present  statisticalmeans to summarize and compare performance measurement across variants.We have evaluated our methodology with a case study of two configurable softwaresystems (GNU XZ and x264). Using our methodology, for both software systems, wewere able to obtain a performance evolution history  of around a decade each.We identified common patterns, among others a direct relationship betweenperformance degradation (execution time measurements increase) and revisionsindicating the introduction of new functionality. In addition, we observedperformance improvements for revisions indicating bug-fixes, and refactorings.However, both software systems exhibited different global trends regardingtheir performance evolution history which suggests different levels ofmaturity. As a more mature configurable software system, for x264, all testedvariants evolved homogeneously, while for GNU XZ, most tested variants evolvedheterogeneously and independent from each other. In fact, a subsequentreliability assessment of the performance measurement tool indicated thatmeasurement spread was merely dependent on the software system tested, and waslower for the more mature software system x264.\paragraph{Research Contributions and Limitations.} We contribute amethodological framework to obtain performance measurements for variant- andversion-rich software systems. We believe to provide practitioners and theresearch community with an integrated and comprehensive set of guidelines toanalyze and understand a configurable software system’s performance in twodimensions, time and variability. Besides the methodological guidelines, weprovide the implementation of our experiment setup, used sampling strategies,and performance measurement results for GNU XZ and x264 at\url{github.com/smba/SPLPioneerPublic} to enable further research and work inthis field.As we have learned from the evaluation of version sampling strategies as wellas the overall evaluation, the software system tested can have a not negligibleimpact on the methodology accuracy. Therefore, we push for more research thatis still required to render precisely the impact for software systems ofdifferent levels of maturity and from different domains.\section{Outlook and Future Work}In the course of this thesis, several aspects arose that can be taken intoaccount for future work on the assessment of performance evolution forconfigurable software systems. For the revision sampling strategy changed-filessampling (cf. section\,\ref{sec:changedfiles}), we have learned and estimatedthe impact of modifying a single file on the software system’s performance. Regarding thissampling strategy driven by machine-learning, we propose the following possibleextensions that we believe sketch possible future directions. First, thecurrent features mapped to the performance influence are files. While thesampling strategy with this level of granularity can be easily applied toarbitrary software systems, more fine-grained feature-to-impact mappings arepossible. For instance, instead of files, functions or methods could beconceived as features to map to performance impact estimates. Although thisrequires additional  language-specific parsing, we believe this to be apromising extension that might further increase the accuracy of this samplingstrategy. Second, the learned knowledge about the impact of modifying aspecific file (or function) can be used to localize those code sections thatare likely to impact performance. This knowledge, for instance, be used toadvise future developers to be aware of the possible impact of modifying acertain file. This might sketch a good basis for possible extensions tointegrated development environments.