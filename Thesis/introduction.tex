\section{Context}
\subsection{Configurable Software}
Modern software systems often need to
be customized to satisfy user requirements. Customizable software, for instance,
enable greater flexibility in supporting varying hardware platforms or tweaking
system performance. To make software systems configurable and customizable, they
exhibit a variety of \emph{configuration options}, also called
\emph{features} \citep{apel_feature-oriented_2013}.
Configuration options range from fine-grained options that tune small
functional- and non-functional properties to those that enable or disable entire
parts of the software. The selection of configuration options can be
accommodated at different stages: \emph{compile-} or \emph{build-time} when the
software is built or at \emph{load-time} before the software is actually used.
Compile-time variability usually governs what code sections get
compiled in the program. For instance, compile-time variability can be
realized by excluding code sections from compilation using preprocessor
annotations \citep{hunsen_preprocessor-based_2016} or by assembling the code
sections to compile incrementally from delta modules
\citep{schaefer_delta-oriented_2010}. In contrast to that, load-time variability
controls which code sections can be visited during execution. Configurations for
load-time variability can be specified using configuration files, environment
variables or command-line arguments. Examples for configurable software systems
range from small open-source command-line tools to mature ecosystems including
Eclipse or even operating systems such as the Linux kernel with more than
$11,000$ options \citep{dietrich_robust_2012}.

Configuration options for
software systems are usually constrained (e.g., are mutually exclusive, imply
or depend on other features) to a certain extent. In the worst case though,
where all options can be selected independently, the number of valid
configurations grows exponentially with every feature added and likely exceeds
the number of atoms in the entire universe once we count $265$ independent
features. Hence, even for a small number of features, any naive approach for
assessing emergent properties of configurable software systems exhaustively for
each valid configuration is in general conceived infeasible. Despite this
mathematical limitation, many feasible approaches to static analysis for highly
configurable systems emerged. Those variability-aware approaches enable, for
instance, type checking in the presence of variability by exploiting
commonalities among different variants \citep{thum_classification_2014}.

To meet functional and non-functional requirements, users aim at finding the
optimal configuration of a configurable software system. However, this task is
non-trivial and has shown to be NP-hard \citep{white_selecting_2009}. The main
driver for the complexity are feature interactions. A \emph{feature
interaction} is an ``emergent behavior that cannot be easily deduced from the
behaviors associated with the individual features involved''
\citep{apel_feature-oriented_2013} and can make development and maintenance of
a configurable system an error-prone task.

{\color{red}
A commonly referred example of a feature interaction, as drafted by
\cite{calder_feature_2003}, describes interactions in telecommunication networks. 
Given two independent features \textsf{CallForwarding} and
\textsf{CallWaiting}, where \textsf{CallForwarding} forwards a call from a busy
line to a line that is available, and where \textsf{CallWaiting} notifies a
busy line when a call is on hold. In isolation their behavior is well-defined,
but if both features are selected, their oppositional behavior becomes
problematic. If no precedence of one feature has been specified, the network
might end up in race conditions or other unexpected behavior. That is, to avoid
this feature interaction, for instance, precedence constraints must be
implemented or the selection of both features must be mutually exclusive.}

\subsection{Performance Behavior}
The term \emph{performance} with respect to software and software systems is not
precisely defined and differs from a user's and a developer’s perspective.
According to \cite{molyneaux_art_2014}, from a user’s perspective ``a well-performing
application is one that lets the end user carry out a given task without any
undue perceived delay or irritation''. However, to accurately assess
performance, from a practitioner’s perspective, performance is outlined by
measurements called \emph{key performance indicators} (KPIs) relating to
non-functional requirements \citep{molyneaux_art_2014}. KPIs in general include
availability of a software system, its response time, throughput, and resource
utilization. Availability comprises the amount of time an application is
available to the user. Response time describes the amount of time it takes to
process a task. Throughput describes the program load or number of items passed
to a process. Resource utilization describes the used quota of resources used
for processing a task.

Performance of a software system depends on the functionality offered, the
respective implementation, program load, the underlying hardware system,
environment variables, and the resulting execution. Since configuration options
control what and how functionality is executed, we concentrate here on those
source of performance. While feature interactions not necessarily cause the
software system to break severely in all cases, its overall performance can
become unfavorable for corner cases or specific configurations as the feature
selection influences the execution \citep{foo_mining_2010,heger_automated_2013,nguyen_industrial_2014}. 
That is, the choice of features as well shapes the performance of a software system.

\subsection{Modeling Performance Behavior}

The aspect of performance of configurable software systems has gained more
attention recently, even though from a practitioners view, according to
\cite{molyneaux_art_2014}, for the most part, performance testing is still not
accommodated to an acceptable degree in the development process.
Assessing performance for configurable systems incorporates obtaining knowledge about the performance of
every valid configuration. 

In the recent years, a variety of approaches to
model, learn, and predict performance behavior of configurable software system
have emerged. The scheme behind these approaches is the conception of
performance modeling as an optimization problem, i.e., to recover and
approximate performance behavior as a function of the selection of
configuration options.
Genetic algorithms \citep{guo_genetic_2011,sayyad_scalable_2013}  have shown
reasonable results, yet are not able to handle constraints such as mutual
exclusion.
\cite{siegmund_predicting_2012} proposed a method to predict performance for arbitrary variants following an approach for
automated detection of feature interaction \citep{siegmund_predicting_2012}.
Following their approach, in 2015 they proposed performance-influence models as a means
to analyze and predict performance for configurable software systems
\citep{siegmund_performance-influence_2015}. A performance-influence model
attempts to approximate the influence of both single features and interacting
features on the software systems' performance.
The approach has shown a reasonably low error rate for several real-world
applications and allows prediction of system performance for arbitrary
configuration variants.

\subsection{Performance of Evolving Software}

Going a step further, actively maintained software systems evolve and so
configurable systems. As software evolves by adapting to changed
contexts and requirements \citep{peng_analyzing_2011}, changes usually
introduce new functionality to the system. Moreover, functionality might be
divided into smaller modules to enable provide more fine-grained configuration
options. Finally, features might be removed from the software system. The
corresponding functionality might remain in the code base or options are merged
\citep{apel_feature-oriented_2013}.  While there
exists substantial work on understanding the evolution of configurable systems,
for example, documenting common symptoms of architectural decay
like code scattering and tangling \citep{passos_feature_2015,zhang_variability_2013} or attempting to
classify patterns for variability evolution
\citep{seidl_co-evolution_2012,peng_analyzing_2011,passos_towards_2012}, there
is little reseach addressing the evolution of performance or non-functional
properties in configurable systems. The evolution of software may affect the
overall quality of a software system and this, in turn, might affect the
performance behavior of its variants. The phenomenon of degrading performance
behavior is commonly referred to as \emph{performance regression}.

To get a better understanding about software evolution and to address
performance regression problems, it is inevitable to continue
studying the performance and performance evolution of configurable systems. In practice, all
aforementioned approaches to model and predict performance behavior for a
configurable software system require exhaustive records of performance
measurements to learn from. Even though valid configurations can be sampled to
some extent \citep{apel_feature-oriented_2013,siegmund_performance-influence_2015}, assessing a single version
of a configurable software system still demands a large number of valid configurations to be measured. In
addition, to study the performance evolution of configurable software systems,
a history or series of performance models is required. That is, assessing
performance evolution of configurable systems is infeasible without
automated tool support.

\section{Problem Statement}
The assessment of performance evolution requires a series of performance models
describing performance behavior for a series of versions of a configurable
systems. Assessing the performance behavior for a single version of a
configurable software system entails a number of necessary and preliminary
tasks. These tasks can even become more complicated once a instead of a single
version a series of versions is assessed.

\emph{First}, not all configurable systems do explicitly exhibit a variability
model what is required to derive all valid variants. While substantial work exists on
reverse engineering variability models from source code or non-code artifacts,
most techniques still involve manual decisions and domain knowledge. Moreover,
variability models evolve as part of the software,  may vary from version to
version and, thus, require multiple reverse engineering steps.
\emph{Second}, the translation from a valid configuration to a configuration
artifact such as a configuration file or a list of command-line arguments may differ
from system to system. This step may be automated, but one still needs to
detect how configurations are read by the software system one wants to study.
\emph{Third}, same goes for the infrastructure to compile or build a software
system since there exist multiple build tools such as makefiles or sbt. Again, the
build process can be automated, but one needs to detect the build mechanism
used.
\emph{Fouth}, to study performance evolution one needs to specify which
snapshots or versions of a software system one wants to study. While detecting
releases and release candidates should be straightforward, one might, for
instance, be interested in the performance evolution including snapshots
between two releases. As not all snapshots though are likely to compile,
classifying defect snapshots can still be tedious work.
\emph{Finally}, the accurate assessment of performance evolution requires an
appropriate testing setup. The methodology required for assessing performance
among others requires the selection of suitable performance indicators and
corresponding benchmarks, means to record measurements and repeat experiments
easily, and proper ways to interpret and compare results.

\section{Goals and Thesis Structure}

The goal of this thesis is to provide a theoretical and practical foundation for
exhaustive performance measurements of configurable software systems and series
thereof. We contribute a guideline of and tool support for performance
measurements for configurable and evolving software systems. Our research
objectives and desired outcomes are

\begin{itemize}
\item a literature overview and discussion regarding software evolution and
performance testing, especially with respect to the presence of variability,
\item a methodology to assess performance evolution with respect to the
aforementioned challenges, and
\item a practical tool for performance measurement for multiple revisions of
configurable software systems.
\end{itemize}

The Thesis is organized as follows. Chapter \ref{chapter:2} provides the
background to the relevant topics discussed in this thesis, including software
evolution, the foundations and statistical aspects of performance testing,
variability model synthesis, and recent approaches to performance modeling. In
Chapter \ref{chapter:3}, we propose our measurement methodology and discuss
the methods used for our performance measurement tool as well as its
limitations. In Chapter \ref{chapter:4}, we evaluate several aspects of our
tool with respect to practicality and discuss the results thereof. Finally,
Chapter \ref{chapter:5} concludes the thesis and gives an outlook on possible
future work.
