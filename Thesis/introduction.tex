\section{Context}
\subsection{Configurable Software}
Modern software systems often need to
be customized to satisfy user requirements. Customizable software, for instance,
enables greater flexibility in supporting varying hardware platforms or tweaking
system performance. To make software systems configurable and customizable, they
exhibit a variety of \emph{configuration options}, also called
\emph{features} \citep{apel_feature-oriented_2013}.
Configuration options range from fine-grained options that tune small
functional- and non-functional properties to those that enable or disable entire
parts of the software. The selection of configuration options can be
accommodated at different stages: \emph{compile-} or \emph{build-time} when the
software is built or at \emph{load-time} before the software is actually used.
Compile-time variability usually governs what code sections get
compiled in the program. For instance, compile-time variability can be
realized by excluding code sections from compilation using preprocessor
annotations \citep{hunsen_preprocessor-based_2016} or by assembling the code
sections to compile incrementally from delta modules
\citep{schaefer_delta-oriented_2010}. In contrast to that, load-time variability
controls which code sections can be visited during execution. Configurations for
load-time variability can be specified using configuration files, environment
variables or command-line arguments. Examples for configurable software systems
range from small open-source command-line tools to mature ecosystems including
Eclipse or even operating systems such as the Linux kernel with more than
$11,000$ options \citep{dietrich_robust_2012}.

Configuration options for
software systems are usually constrained (e.g., are mutually exclusive, imply
or depend on other features) to a certain extent. In the worst case though,
where all options can be selected independently, the number of valid
configurations grows exponentially with every feature added and likely exceeds
the number of atoms in the entire universe once we count $265$ independent
features. Hence, even for a small number of features, any naive approach for
assessing emergent properties of configurable software systems exhaustively for
each valid configuration is in general conceived infeasible. Despite this
mathematical limitation, many feasible approaches to static analysis for
configurable systems emerged. Those variability-aware approaches enable, for
instance, type checking in the presence of variability by exploiting
commonalities among different variants \citep{thum_classification_2014}.

To meet functional and non-functional requirements, users aim at finding the
optimal configuration of a configurable software system. However, this task is
non-trivial and has shown to be NP-hard \citep{white_selecting_2009}. The main
driver for the complexity are feature interactions. A \emph{feature
interaction} is an ``emergent behavior that cannot be easily deduced from the
behaviors associated with the individual features involved''
\citep{apel_feature-oriented_2013} and can make development and maintenance of
a configurable system an error-prone task.

To illustrate feature interactions, consider the following example
\citep{siegmund_performance-influence_2015}. A software system, say a file
server, is used to store in a data base and provide access upon request. The
system provides functionality for both encryption and compression. In
isolation, both file en- or decryption and file (de-)compression demand an
expectable fraction of memory and processor time. The performance behavior for
the software system though may vary if both features are selected. For
instance, if a file is encrypted and compressed (or vice versa), we can expect
the operation to demand less resources since an encrypted file is likely to be
of smaller size than the decrypted original.
This is a positive example for a feature interaction, where the performance
behavior although being beneficial is unexpected.

\subsection{Performance Behavior}
Performance with respect to software and software systems is not
precisely defined and differs from an end user's and a developer’s perspective.
According to \cite{molyneaux_art_2014}, from a user’s perspective ``a well-performing
application is one that lets the end user carry out a given task without any
undue perceived delay or irritation''. However, to accurately assess
performance, from a practitioner’s perspective, performance is outlined by
measurements called \emph{key performance indicators} (KPIs) which relate to
non-functional requirements \citep{molyneaux_art_2014}. The set of KPIs include
availability of a software system, its response time, throughput, and resource
utilization. Availability comprises the amount of time an application is
available to the user. Response time describes the amount of time it takes to
process a task. Throughput describes the program load or number of items passed
to a process. Resource utilization describes the used quota of resources used
for processing a task.

The performance behavior of a software system depends on the functionality
offered, the respective implementation, program load, the underlying hardware system,
environment variables, and the resulting execution. Since configuration options
control what and how functionality is executed, we concentrate here on those
source of performance. While feature interactions not necessarily cause the
software system to break severely in all cases, its overall performance can
become unfavorable for corner cases or specific configurations as the feature
selection influences the execution \citep{foo_mining_2010,heger_automated_2013,nguyen_industrial_2014}. 
That is, the choice of features as well shapes the performance of a software system.

\subsection{Modeling Performance Behavior}
{\color{orange}
The aspect of performance of configurable software systems has gained more
attention recently, even though from a practitioners view, according to
\cite{molyneaux_art_2014}, for the most part, performance testing is still not
accommodated to an acceptable degree in the development process.

Even though, from a practicioners' perspective, performance testing is still not
accommodated to an acceptable degree in the development process, the assessment
of performance of configurable software systems has gained more attention
recently. As assessing performance for configurable systems incorporates
obtaining knowledge about the performance of every valid configuration, in the
recent years, a variety of approaches to model and predict performance
behavior of configurable software system have emerged. The scheme behind these
approaches is the conception of performance modeling as an optimization
problem, i.e., to recover and approximate performance behavior as a function of
the selection of configuration options.
Genetic algorithms \citep{guo_genetic_2011,sayyad_scalable_2013}  have shown
reasonable results, yet are not able to handle constraints such as mutual
exclusion.
\cite{siegmund_predicting_2012} proposed a method to predict performance for
arbitrary variants, following an approach for automated detection of feature interactions \citep{siegmund_predicting_2012}.
Moreover, in 2015 they proposed performance-influence models as a means
to analyze and predict performance for configurable software systems
\citep{siegmund_performance-influence_2015}. A performance-influence model
attempts to approximate the influence of both single features and interacting
features on the software systems' performance.
The approach has shown a reasonably low error rate for several real-world
applications and allows prediction of system performance for arbitrary
configuration variants.}

\subsection{Performance of Evolving Software}

Actively maintained (configurable )software systems evolve by adapting to changed
contexts and requirements \citep{peng_analyzing_2011}. Changes usually
introduce new functionality to the system, but functionality might as well be
divided into smaller modules to enable provide more fine-grained configuration
options. When features are removed from the software system, the
corresponding functionality might remain in the code base or options are merged
\citep{apel_feature-oriented_2013}.  While there
exists substantial work on understanding the evolution of configurable systems,
for example, documenting common symptoms of architectural decay
like code scattering and tangling \citep{passos_feature_2015,zhang_variability_2013} or attempting to
classify patterns for variability evolution
\citep{seidl_co-evolution_2012,peng_analyzing_2011,passos_towards_2012}, there
is little reseach addressing the evolution of performance or non-functional
properties in configurable systems. {\color{red}The evolution of software may
affect the overall quality of a software system and this, in turn, might affect the
performance behavior of its variants. The phenomenon of degrading performance
behavior is commonly referred to as \emph{performance regression}.}

To get a better understanding about software evolution and to address
performance regression problems, it is inevitable to continue
studying the performance and performance evolution of configurable systems. In practice, all
aforementioned approaches to model and predict performance behavior for a
configurable software system require exhaustive records of performance
measurements to learn from. Even though valid configurations can be sampled to
some extent \citep{apel_feature-oriented_2013,siegmund_performance-influence_2015}, assessing a single version
of a configurable software system still demands a large number of valid configurations to be measured. In
addition, to study the performance evolution of configurable software systems,
a history or series of performance models is required. That is, assessing
performance evolution of configurable systems is infeasible without
automated tool support.

\section{Problem Statement}
The assessment of performance evolution requires a series of performance models
describing performance behavior for a series of versions of a configurable
system. Assessing the performance behavior for a single version of a
configurable software system entails a number of necessary and preliminary
tasks. These tasks can even become more complicated once instead of a single
version a series of versions is assessed:
\begin{itemize}
  \item \emph{Feature Model Synthesis:} Not all configurable systems do
  explicitly exhibit a variability model what is required to derive all valid variants
	\citep{rabkin_static_2011,nadi_where_2015}.
	While substantial work exists on reverse engineering variability models from
	source
	\citep{rabkin_static_2011,she_reverse_2011,zhou_extracting_2015,nadi_where_2015}
	code or non-code artifacts
	\citep{alves_exploratory_2008,andersen_efficient_2012,bakar_feature_2015}, many
	techniques still involve manual decisions \citep{she_reverse_2011} and domain
	knowledge \citep{nadi_where_2015}.
	Moreover, variability models evolve as part of the software
	\citep{peng_analyzing_2011}, vary from version to version, and, therefore,
	require repeated reverse engineering steps.
	
	\item \emph{Configuration Translation:} the translation of a valid
	configuration to a configuration artifact such as a configuration file or a list of command-line arguments may differ
from system to system. This step may be automated, but one still needs to
detect how configurations are read by the software system one wants to study.

\item \emph{Automated Integration:} Same goes for the infrastructure to
compile or build a software system since there exist may possible build tools such as makefiles or sbt.
Again, the build process can be automated, but one needs to detect and
specify the build mechanism used.

\item \emph{Version History Sampling:} To study performance evolution one
needs to specify which snapshots or versions of a software system one wants to study. While detecting
releases and release candidates should be straightforward, one might, for
instance, be interested in the performance evolution including snapshots
between two releases. As not all snapshots though are likely to compile,
classifying defect snapshots can still be tedious work.
\item \emph{Performance Assessment Setup:} The accurate assessment of
performance evolution requires a suitable testing setup. The methodology required for assessing performance
among others requires the selection of suitable performance metrics and
corresponding benchmarks, means to record measurements and repeat experiments
easily, and proper ways to interpret and compare results.
\end{itemize}






\section{Goals and Thesis Structure}

The goal of this thesis is to provide a theoretical and practical foundation for
exhaustive performance measurements of configurable software systems and series
thereof. We contribute a guideline of and tool support for performance
measurements for configurable and evolving software systems. Our research
objectives and desired outcomes are

\begin{itemize}
\item a literature overview regarding software evolution, feature model
synthesis and performance assessment,
\item a methodology to assess performance evolution with respect to the
aforementioned challenges, and
\item a practical tool for performance measurement for multiple revisions of
configurable software systems.
\end{itemize}

The Thesis is organized as follows. Chapter \ref{chapter:2} provides the
background to the relevant topics discussed in this thesis, including
variability modeling, software evolution, the foundations and statistical
aspects of performance testing, and recent approaches to performance modeling. In
Chapter \ref{chapter:3}, we propose our measurement methodology and discuss
the methods used for our performance measurement tool as well as its
limitations. In Chapter \ref{chapter:4}, we evaluate several aspects of our
tool with respect to practicality and discuss the results thereof. Finally,
Chapter \ref{chapter:5} concludes the thesis and gives an outlook on possible
future work.
