To assess performance evolution for configurable software systems, it is
inevitable to assess and understand variability of such systems with respect to
various aspects: \emph{First}, to actually assess single variants, knowledge
about the variability model is required to configure the software systems accordingly.
\emph{Second}, obtaining knowledge about feature usage and implementation can
provide meaningful insights. For instance, knowing that most variable code is dependent
on larger numbers of features might be useful information when selecting a
sampling strategy. Similarly, knowing which feature combinations are frequently
involved in conditioning program functionality and behavior can help understand
configuration-related defects. \emph{Last}, since the number of configurations
for most configurable software systems is infeasible, variability assessment faces
performance- and scalability-related problems. That is, assessments are usually
conducted constrained by limited resources.

This chapter describes the first preliminary steps in our methodology and
addresses the following questions. Section \ref{sec:untangling} reviews the synthesis of
variability models as well as useful analyses of systems' variability; section
\ref{sec:configuration_gen} describes means to generate configurations from variability
models, and section \ref{sec:configuration_sam} discusses different sampling
strategies.

\section{Untangling Variability}\label{sec:untangling}
The idea of designing and developing configurable software systems is driven by
the separation of different concerns, expressed as features of a software
system. A configurable software system is either assembled at compile-time with
respect to its configuration, or behaves according to a configuration read at
load-time. While for both flavors of variability there exist various ways of
realization, research has proposed a variety of variant-generating
implementation techniques for compile-time variability, ranging from simple
preprocessor directives to features as separate modules
\citep{kastner_model_2009}.  A complete survey of means to implement
configurable software systems would likely exceed the scope of this section, yet we intend to discuss in this
section which information regarding variability is required or useful to our
methodology, and how it can be obtained.

\subsection{Family-based Analyses}
As the number of variants for configurable software systems is usually
infeasible, naive analyses of configurable systems are not trivial. Any static
analysis can only assess one variant at a time, as well as dynamic analyses,
which can only follow one execution path. 
In contrast to that, recently,
extended analysis techniques, which are aware of variability of the systems
studied, have emerged \citep{thum_classification_2014}. In particular, these
\emph{family-based} analyses avoid redundant computation, such as visiting a
code section multiple times, and exploit artifacts shared by multiple variants \citep{thum_classification_2014}. Besides more
efficient analysis, family-based methods incorporate knowledge about valid
feature combinations \citep{thum_classification_2014} and, therefore, connect analysis results with
a context, such as feature combinations, for which the findings hold.
Family-based methods have been widely used across various domains and can
provide useful information when assessing configurable software systems.

\paragraph{Variability-aware Parsing.} \cite{kastner_variability-aware_2011}
have proposed a framework, \emph{TypeChef}, to enable the construction of variability-aware parsers. A variability-aware parser, like
ordinary parsers, systematically explores a program to return an abstract
representation of the parsed program. This parse tree, or abstract syntax tree
is the basis for compilers to translate a program, or for further static
analyses including type checking. For a code base with variability expressed by
preprocessor directives, which are evaluated prior to compilation, a
variability-aware parser, however, is able to derive a parse tree considering
all variants in a single run. A parse tree usually consists of nodes
representing syntactical features of the parsed program. The parse tree
returned by a variability-aware parser, moreover, comprehends variable segments
of a program and will include them with respect to their presence conditions.
For instance, a class may contain a function sort, for which two different
implementations exist. While there might be numerous variants, the parse tree
of the class will contain a node with two children, one for each
implementation; higher numbers of alternative implementations are expresses by
nesting further nodes. In that way, variable and invariant program segments
can be separated.

While the approach of \cite{kastner_variability-aware_2011} handles
undisciplined usage of preprocessor directives, such as splitting function parameter lists, variable types, or
expressions, \cite{medeiros_discipline_2017} have proposed an approach to avoid and
conservatively refactor those cases. The authors propose a catalog of
refactoring templates, which describe transformations from undisciplined usage
of preprocessor annotations to disciplined ones. With respect to
variability-aware parsing, disciplined usage is conceived as using preprocessor
annotations only to segment statements, but not to segment a single syntactical
unit, such as expressions \citep{medeiros_discipline_2017}.

\paragraph{Staged Variability.} Besides variability-aware parsing,
\cite{nguyen_building_2014} have applied symbolic execution
\citep{king_symbolic_1976,darringer_applications_1978} to unwind variability for
PHP Web applications. Web applications are staged, i.e., even though it can be configured at load-time, the applications is as well variable with respect to input received at run-time. For instance,
consider WordPress, a popular content management system (CMS) implemented in
PHP, can be extended with a number of plug-ins. However, the content of a
website presented to the user also depends on information retrieved from a
database, and user input. Consequently, a dynamic PHP Web application is staged
in a sense that it generates configurable HTML templates which are rendered at
run-time. The authors utilize a symbolic execution to explore all possible
execution paths. Each user input or database query is considered a symbolic
value which is propagated through each script. By keeping track of the
(partially symbolic) HTML output and organizing it in a DOM-like structure,
their approach approximates the HTML output, which subsequently can be tested,
for instance for validity \citep{nguyen_auto-locating_2011}.

Similarly, \cite{lillack_tracking_2014} have applied taint-analysis to configurable software
systems to track the influence of configuration options read at load-time.
Their static analysis approach taints every value resulting from reading a
configuration parameter as well as every value resulting from a computation
that involves previously tainted values. That way, lines of code that are
possibly depending on configuration options are detected.

\paragraph{Build System Variability.} Build System Variability. Apart from
configuring software systems using preprocessor annotations, the assembling of
a configurable software system can as well be orchestrated by its underlying
build system. While preprocessor annotations virtually separate code fragments
of different features, for instance, build systems can physically exclude files
from compilation. This variability enabled by build systems, in particular of
Makefiles has been subject to a couple of analysis approaches. \cite{tamrawi_build_2012} have
proposed Symake, a symbolic execution engine to evaluate Makefiles.
On top of Symake, \cite{zhou_extracting_2015} use symbolic execution to analyze
Makefiles and derive file presence conditions, stating under which feature
selection a file is included or excluded from compilation. The work of
\cite{al-kofahi_escaping_2016} addresses a more advanced build system, GNU Automake.
Automake describes a staged build process, where a Makefile can be specified on
a higher level, and is subsequently compiled to an actual Makefile. The
authors’ aim to provide a variability-aware representation of all possible
Makefiles to enable further analyses of the build process.

\subsection{Variability Model Synthesis} \label{sec:feature_model_synthesis} 
A variability model as an abstraction of functionality of a software system is
required, or at least of great interest, in many contexts. Not every
configurable system provides an explicit representation of its variability
model. The reasons for inexplicit or absent configuration specification are
manifold. They can range from poor or inconsistent documentation \cite{rabkin_static_2011} to
overly complex configurability \cite{xu_hey_2015}, or configuration constraints originated in different layers of a software system,
such as build constraints or compiler constraints \cite{nadi_where_2015}. The
following paragraphs review different strategies to extract of features and
feature models from different types of artifacts.

\paragraph{NLP-based Extraction.} As feature diagrams group and organize
features (representing functionality), synthesizing a variability model has shown to be applicable to extract features
and constraints from natural language artifacts. For instance, by comparing
product specifications for an existing market domain, variability models can
provide a detailed feature summary \citep{alves_exploratory_2008}.

The basic idea is to identify commonalities and differences in natural language
documents, such as product descriptions, requirements, or documentations, by
using natural language processing (NLP) techniques \citep{bakar_feature_2015}. A
widely employed technique is to conceive a text as a vector in a vector space model, where each word or
token represents a dimension. From the tokenized text, irrelevant stop words
are removed, and all remaining words are reduced to their original word stems.
The importance of all remaining tokens is (usually) weighed by its tf-idf
value, an established technique in information retrieval. That is, each text
(corresponding to a variant or configuration) is represented as a vector of
tf-idf values in the aforementioned vector space model. Based on this
representations, texts can be clustered to identify commonalities and
differences in terms of clusterings. Subsequently, the clustering information
can be used to extract features or entire feature
models \cite{alves_exploratory_2008,bakar_feature_2015}.

%\subsection{Feature Extraction} 
The first objective in recovering a variability model from a configurable
system is to determine the available configuration options to select
from. In addition, for further configuration, the type of each configuration
option (e.g., boolean, numeric, or string) and the respective domain of valid
values needs to be specified.

\paragraph{Feature Extraction.} \cite{rabkin_static_2011} proposed a static, yet
heuristic approach to extract configuration options along with respective types and domains. Their approach
exploits the usage of configuration APIs and works
in two stages. It commences with extracting all code sections where
configuration options are parsed. Next, configuration names can be
recovered as they are either already specified at compile-time or can be
reconstructed using string analysis yielding respective regular expressions.
Moreover, the authors employ a number of heuristics to infer the type of parsed
configurations as well as respective domains. First, the return type of the
parsing method is likely to indicate the type of the configuration option read.
Second, if a string is read initially, the library method it is passed to can
reveal valuable information about the actual type. For instance, a method
\emph{parseInteger} is likely to parse an integer value. Third, whenever a
parsed configuration option is compared against a constant, expression, or value
of an enum class, these might indicate valid values or at least corner cases of
the configuration option domain. The extraction method by
\cite{rabkin_static_2011} is precise, but limited, for instance, when an
option leaves the scope of the source code.
Nonetheless, for the systems studied, the authors were able to recover
configuration options that were not documented, only used for debugging or even not used at
all.

\paragraph{Constraint Extraction.} 
A more comprehensive investigation of configuration
constraints and their origin is provided by \cite{nadi_mining_2014,nadi_where_2015}. They use variability-aware parsing to infer constraints by
evaluating make files and  analyzing preprocessor directives. Inferred
constraints result from violations of two assumed rules, where (a) every valid
configuration must not contain build-time errors and (b) every valid
configuration should result in a lexically different program. While the
first rule aims at inferring constraints that prevent build-time errors, the
second one is intended to detect features without any effect, at least as part
of some configurations. Their analysis one the one hand emerged to be accurate
in recovering constraints with 93\,\% for constraints inferred by the first rule
and 77\,\% for second one respectively. On the other hand, their approach
recovered only 28\,\% of all constraints present in the software system.
Further qualitative investigation, including developer interviews, lead to
the conclusion that most of existing constraints stem from domain knowledge
\citep{nadi_where_2015}.

\paragraph{Feature Model Approximation.}
A different strategy to recover variability models, instead of analyzing the
software artifacts, is to approximate a model. Given a selection of valid
feature selections, a variability model best describing the configurations can
be approximated, or learned. \cite{lopez-herrejon_assessment_2015} have surveyed
different search-based strategies to synthesize feature models of which we
present two categories. Evolutionary algorithms have been applied to reverse
engineer feature models from configuration samples
\citep{lopez-herrejon_reverse_2012,linsbauer_feature_2014}. A population of
feature models is generated and each instance is evaluated by a fitness function, measuring how well it fits the given sample configurations. Subsequently, a new generation is obtained by applying
crossover and mutation operators to the previous generation, whereby only the
fittest remain. This process of evolution is repeated multiple times until a
desired threshold fitness is reached for a feature model instance.
\cite{lopez-herrejon_reverse_2012} identify a trade-off between the accuracy of
recovered feature models and the number of generations employed by evolutionary
algorithms. Besides promising results, the authors stress the importance of
effective and scalable fitness functions as well as meaningful samples to learn
the feature model from.
Contrary to evolutionary algorithms,
\cite{haslinger_reverse_2011,haslinger_extracting_2013} have proposed an ad-hoc
algorithm to reverse engineer feature models. The algorithm recovers
the feature model layer by layer by  extracting for a parent feature all child
features recursively. The algorithm does not consider cross-tree constraints.
Besides promising results for basic feature models, the authors advocate the
incorporation of human domain-knowledge in the synthesis of feature models.
 
\paragraph{Feature Hierarchy Recovery} \label{sec:feature_hierarchy}
Besides recovering configuration options and respective constraints, to reverse
engineer a feature model, one further step is required. The recovered knowledge
needs to be organized in a tree-like hierarchy with feature groups specified and
CTCs explicitly stated to derive a valid feature diagram 
\citep{kang_feature-oriented_1990}.
While several approaches for recovering the feature model hierarchy have been
proposed, we are primarily interested in finding a hierarchy for knowledge
obtained from source code. Other scenarios, as already stated in the opener of
this section, are based on product descriptions or sets of valid configurations
\citep{aleti_software_2013,bakar_feature_2015}. In the remainder of this
subsection, we will focus on organizing features and constraints extracted from
source code. For further reading, \cite{andersen_efficient_2012} present algorithms for structuring feature diagrams for three different scenarios including the ones previously mentioned.

Given an extracted set of features along with corresponding descriptions and
recovered constraints among the features, \cite{she_reverse_2011} propose a
semi-automated and interactive approach to synthesize a feature hierarchy.
Their approach comprises three tasks. First, an overall feature hierarchy based
on feature implications is specified. Second, potential feature groups are
detected and manually selected. Finally, the feature diagram is extended with
remaining CTCs. 
The approach by \cite{she_reverse_2011} provides a practical algorithm to synthesize a
feature diagram, yet has some limitations we need to consider. First, the
approach is not able to detect or-groups as defined in Sec. \ref{sec:variability_modeling}.
Second, the approach does introduce a root feature. Finally, the approach does not
distinguish between mandatory and optional features. Implicitly, all features
that do not have a parent feature are optional and all features that have a
parent feature are by default mandatory. \cite{she_reverse_2011} evaluated the
algorithm with both complete and incomplete variability knowledge (feature
names, descriptions and constraints). While the algorithm has shown to be
practical, detecting features whose parent was the root-feature was difficult
since, due to the transitive property of implication, it is implied by each
feature of the feature model.

%\begin{enumerate}
%  \item The algorithm commences with finding a single parent for each
%  feature and, thus, specifying a tree-like feature hierarchy. Based on the
%  given constraints, a directed acyclic graph (DAG) representing implication
%  relationships among features, a so-called \emph{implication graph}, is
%  constructed.
%  Every vertex in the implication graph represents a feature  and edges are
%  inserted for each pair of features $(u, v)$, where  $u \implies v$ holds with
%  % respect to the given constraints.
%   
%  In addition to the implication graph, the algorithm for each feature computes
%  two rankings of features that are likely to be the respective parent feature.
%  The two rankings both employ the feature descriptions. Feature descriptions
%  are compared for similarity using a similarity metric. For two features $p$
%  and $s$, the similarity is defined as the weighted sum of the inverse
%  % document frequencies $idf(w)$ for the words that both descriptions of
% features $p$%  and $s$ share.

%  The $idf$-ranking for a word $w$ is the logarithm of the number of features
%  divided by the number of features whose description contains $w$. Each $idf$
%  value is weighted with the frequency of $w$ in the description of
%  feature $p$.
  
%  The first ranking, called Ranked-Implied-Features (RIF), for each feature $f$
%  ranks all features by their similarity to $f$ in an descending order, but
%  prioritizes those features that are implied according to the previously
%  computed implication graph. The second ranking, called Ranked-All-Features
%  (RAF) is similar to RIF, yet less strict since implied features are not
%  prioritized. Given these rankings, a user for each feature selects a suitable
%  parent feature from the RIF or RAF ranking. The idea behind providing two
%  separate rankings, according to \cite{she_reverse_2011} is that the given
%  extracted constraints can be incomplete and, thus, not all relevant
%  implications are contained in the implication graph.

%  \item After the feature hierarchy is specified, another auxiliary graph, a
%  mutex graph, similar to the implication graph, is constructed. The
%  % \emph{mutex graph} is an undirected graph with features as vertices and
%  % edges between two
%  features $u$ and $v$, if $u \implies \neg{v}$ and $v \implies \neg{u}$ hold
%  with respect to the given constraints. 
%  That is, all adjacent features are mutually exclusive. Based on
%  this mutex graph, all maximal cliques (subsets of vertices that all are
%  connected with each other) among the vertices with the same parent are
%  computed. All features within such a clique are mutually exclusive and share
%  the same parent and represent mutex- or alternative-groups.
  % \cite{she_reverse_2011} introduce an additional constraint to extract xor-groups that require one of the groups’
%  features to be selected if the parent is selected. This distinction is in
%  line with the initial description of feature diagrams by
  % \cite{kang_feature-oriented_1990}, but not all descriptions of feature diagrams follow this distinction between
%  mutex- and xor-groups and just use the term alternative-group discussed in 
%  Sec. \ref{sec:variability_modeling}. %2.1
  
 % \item CTCs for the feature diagram are extracted from
 % the given configuration constraints. Since CTCs are constraints that could
 % not be represented by the feature hierarchy (implication) or
 % alternative-groups (exclusion), the derivation of CTCs follows this idea. The
%  set of cross-tree implications is derived by removing all edges that are part
%  of the feature hierarchy from the initially constructed implication graph.
%  The set of cross-tree exclusions is derived similarly from the mutex
%  graph by removing all edges among vertices of all mutex-groups. To make the
%  feature model sound, the given configuration constraints, reduced to those
%  clauses that are not already entailed by the diagram, can be added as an
%  additional CTC formula to the feature diagram \citep{she_reverse_2011}.
%\end{enumerate}

\subsection{Practical Remarks}
The last two subsections reviewed a number of family-based analyses for
configurable software systems as well as approaches proposed to partially
extract variability models from a system’s code base. The latter approaches
presented, however, are rather isolated solutions due to non-generic
assumptions, such as the use of configuration APIs \citep{rabkin_static_2011},
or build-time variability \citep{nadi_where_2015}. At best, these approaches
complement each other, or are an appropriate choice under certain
circumstances, still requiring further manual assessment. This subsection
integrates the previously discussed work and proposes methodological strategies
to synthesize variability models for different scenarios, or use cases.

For the recovery of variability models, we propose three scenarios. The
following distinction is neither complete, nor exclusive, as scenarios may be
approached with different strategies. Nonetheless, the scenarios should provide
a practical context to the previously mentioned techniques. The three
scenarios are outlined in Table \ref{tab:synthesis}; we derive our scenarios
based on three criteria. \emph{First}, we ask whether, and if so, to which
extent configurability for a software system is documented. \emph{Second}, we ask
whether the software system provides sample configurations, such as configuration presets, or whether it ships as different variants, such as
different Windows flavors. \emph{Last}, we ask whether a variability model is
explicitly contained in the software, and whether it is visible to
practitioners, such as the Kconfig system for the Linux kernel. For each
scenario, in Table \ref{tab:synthesis}, satisfaction of either criterion is
marked. In addition, we mark criteria as \emph{satisfied optionally}, if we
assume them to be satisfied, but they are not necessarily relevant for the
choice of strategy.

\begin{table} 
	\centering
	\begin{tabular}{lccc}%
	\toprule
	\textbf{Criterion} & \textbf{Scenario A} & \textbf{Scenario B} &
	\textbf{Scenario C}
	\\
	\midrule
	\mbox{Configurability documentated?} & $\surd$ & $(\surd)$ & $(\surd)$ \\
	\mbox{Configurations provided?} & $\times$ & $\surd$ & $(\surd)$ \\
	\mbox{Variability model provided?} & $\times$ & $\times$ & $\surd$ \\
	\bottomrule
	\end{tabular}\\
	\vspace{1mm}
	{\footnotesize $\surd = \text{Criterion satisfied}$, $(\surd) =
	\text{Criterion satisfied optionally}$, $\times = \text{Criterion not
	satisfied}$}
	\caption{Distinction of three scenarios for variability model synthesis. }
	\label{tab:synthesis}
\end{table}

Apparently, the latter scenario C in Table \ref{tab:synthesis} only requires
little to no assessment, as the variability model is already somehow represented in the implementation.
Whereas, the former two scenarios A and B require more assessment.
For scenario A, both natural language artifacts and code artifacts can be
exploited. A basic start is to study and aggregate documentation information
about the software system with respect to configurations, such as manuals/man
pages or API documentation. While there is no template strategy to analyze
documentation artifacts manually, the key questions to be answered are:

\begin{itemize}
  \item \emph{How is variability implemented?} Variability can be accommodated at
  build- or load-time, and can be implemented in various ways.
  \item \emph{How can the software be configured?} Configurations are read
  during the build process, for instance as preprocessor annotations or compiler
  flags; at load-time configurations are read, for instance as command-line
  arguments.
  \item \emph{Which configuration options exist, including type and domain?}
  Configuration options can be binary or numeric options with respective
  domains.
  \item \emph{Which configuration options address the same operation mode?} Do
  configuration options only influence one operation mode, such as compression,
  but not decompression for a file compression utility?
  \item \emph{Which configurations are irrelevant for performance assessment?}
  Configuration options may not influence the overall performance significantly,
  such as a ``keep input file after processing'' option.
  \item \emph{Which dependencies or exist between and among configuration
  options?}
\end{itemize}
   
For scenario B, despite documentation might be available, the
previously presented variability model approximation approaches
\citep{haslinger_reverse_2011,haslinger_extracting_2013,lopez-herrejon_reverse_2012,linsbauer_feature_2014}
can be applied, yet only binary configuration options are supported so far.
Given the existence of sample configuration or configuration presets, these
may provide additional information to answer the questions stated above. 
The remaining approaches mentioned in the previous subsections, unfortunately,
describe only isolated solutions. Given suitable circumstances, they can
nevertheless aid the extraction of variability models. The overall scheme in
synthesizing a variability model is to answer the the questions above manually,
and refer to automated tool support whenever possible.



\section{Configuration Generation}\label{sec:configuration_gen}
Apart from having knowledge about the variability of a configurable software
system, either in the form of a set of constraints among features, or a feature
diagram, we also need to derive all configurations, but at least meaningful
sample sets thereof, from the variability model. Variability models can be
expressed in various forms, such as propositional formulas or context-free
grammars (CFG) \citep{batory_feature_2005}, as well as constraint satisfaction
problems (CSP) \citep{benavides_automated_2005,benavides_using_2005}.
Accordingly, the all configurations represent solutions to propositional formulas or CSPs, and valid words for CFGs
respectively. That is, the generation of all configurations with respect to the
variability model is equivalent to finding a solution or word set for the
aforementioned representations of a variability model. In the following, we
look into how variability models can be encoded as a CSP and describe in detail
the configuration generation using CFGs.

\subsection{Constraint Satisfaction Problem}
A constraint satisfaction problem (CSP) in the context of variability modeling
describes a set of options ranging over finite domain as well as a set of
constraints which restrict which values variables can take simultaneously
\citep{benavides_automated_2005}. For a binary option $b$, the respective domain
$dom(b)$ simply is $\lbrace 0, 1\rbrace$.
For a numeric option, the respective domain $dom(n)$ is a finite set of legal
values with a minimum and a maximum value, say $\lbrace v_{min}, v_1,
v_2, \ldots, v_{max}\rbrace$.
A solution $s: O \rightarrow dom(o_1) \times dom(o_2) \times \ldots \times
dom(o_{|O|})$ to a CSP is an assignment of options $o_i \in O, i \in \mathbb{N}$
to values of their respective domain, such that all constraints are satisfied simultaneously \citep{benavides_automated_2005}.  

A solution to a CSP is found by systematically checking for different
selections of values whether all constraints are satisfied. There exists a
large number of ready-to-use SAT and CSP solvers, yet we are not covering CSP
solution here since it is beyond the scope of your thesis. For further reading, 
\citep{benavides_fama:_2007} present a tool with extensive analysis support for
various different presentations of variability models.

To encode a variability model as a CSP, \cite{benavides_automated_2005} describe
the following transformation rules:

\begin{itemize}
  \item For a parent feature $f$ and a child feature $f’$, a mandatory
  relationship is expressed as $f \Leftrightarrow f’$, and an optional relationship is
  expressed as $f’ \implies f$.
    \item  For a parent feature $f$ and child features $f_i$, where $i = 1, 2,
    \ldots n$, an or-group is expressed as $f \Leftrightarrow f_1 \lor f_2 \lor
    \ldots \lor f_n$, and an alternative-group is expressed as $$\bigwedge_{i
    = 0}^n f_i \Leftrightarrow (f \land \bigwedge_{j \in \lbrack 0, n \rbrack
    \setminus i} \neg f_j) $$.
\end{itemize}

To also consider numeric options, the domain of a numeric option $n$ can be
conceived as an alternative-group since only one value from the domain can be
selected at a time. Hence, each value of the domain $dom(n)$ can be conceived as
a binary option. If value $v \in dom(n)$ is selected, this states $n = v$,
otherwise $n \neq v$.


\subsection{Grammar Expansion}
Besides trying to find solution for satisfiability problems, the expression of
variability models as context-free grammars (CFGs) enables the derivation of
configurations directly from a CFG by expanding it. A
first description of transformation rules, yet only for feature diagrams with
binary options, was was proposed by \cite{batory_feature_2005}. A hierarchical feature diagram
can be recovered from a set of constraints using the algorithm of
\cite{she_reverse_2011} as explained in section \ref{sec:feature_hierarchy}.
In the following we describe
how a feature diagram with both binary and numeric features can be transformed
to a context-free grammar, and how configurations can be derived subsequently.

\begin{definition}[Context-free Grammar]\label{def:cfg}
A context-free grammar is a tuple $G = (N, T, S, P)$, consisting of a set of
non-terminal symbols $N$, a set of terminal symbols $T$, a start word $S \in (N
\cup T)^*$, and a set of productions $P \subseteq N \times (N \cup T)^*$. The
set $L_G$ describes the language of the grammar $G$ and comprises all valid
words $w \in L_G$ which can be derived from the start word $S \in L_G$ by
applying productions a finite number of times to it.
\end{definition}

Following the Definition.~\ref{def:cfg}, to derive all configurations for a
given feature diagram, the idea is to first translate it to a context-free
grammar. In order to do so, especially with respect to handling numeric
options, we introduce an extended definition for a CFG, a configuration
generation grammar.

\begin{definition}[Configuration Generation Grammar]\label{def:cgg}
A configuration generation grammar is a context-free grammar $G = (N, T, S,
P)$ whose elements are constructed from a feature diagram as follows.

\begin{itemize}
  \item All features represent non-terminal symbols $N$, which can be divided
  into two disjoint sets, binary non-terminal symbols $F_\mathcal{B}$ and
  numeric non-terminal symbols $F_\mathcal{N}$. That is, $N = F_\mathcal{B}
  \cup F_\mathcal{N}$ and $F_\mathcal{B} \cap F_\mathcal{N} = \varnothing$.

  \item Similarly, the set of terminal symbols $T$ is consists of two different
  sets, the binary terminal symbols $T_\mathcal{B}$ and the numeric terminal
  symbols $T_\mathcal{N}$, so that $T = T_\mathcal{B} \cup T_\mathcal{N}$ and
  $T_\mathcal{B} \cap T_\mathcal{N} = \varnothing$ with
  
  \begin{equation}
  T_\mathcal{B} = \bigcup_{b\in F_\mathcal{B}} \lbrace b_0,  b_1\rbrace
  \end{equation}
  
  \begin{equation}
  T_\mathcal{N} = \bigcup_{n\in F_\mathcal{N}} ~ \bigcup_{v_i \in dom(n)}
  \lbrace n_{v_i}\rbrace.
  \end{equation}
  
  \item All productions $P$ are constructed from the hierarchy specified in the
  given feature diagram, the binary, and the numeric features. In our definition
  of a configuration grammar, each word $w$ is expressed as a subset of
  (non-)terminal symbols, i.e., $w \subseteq (N \cup T)$. A word is a terminal
  word, if and only if it does not contain any non-terminal symbol. Accordingly,
  the set of productions is $P \subseteq N \times (N \cup T)$, and a production
  $p = (u, v) \in P$ is applied to a word $w$ by removing non-terminal symbol
  $u$ from word $w$ and merging words $v$ and $w$. Hence, the new word $w'$ is
  defined as $w' = (w \setminus u) \cup v$.

  The productions $P = P_H \cup P_F$ are constructed from the following disjoint
  two sets of productions:
  
  \begin{equation}
  P_H = \lbrace (p, \lbrace c, c_1 \rbrace) | p, c \in N \land
  c \implies p\rbrace
  \end{equation}
  
  \begin{equation}
  P_F = \bigcup_{f \in (F_\mathcal{B} \cup F_\mathcal{N})} ~ \bigcup_{v \in
  dom(f)} \lbrace (f, v) \rbrace
  \end{equation}
	
  \item Finally, the start word $S \subseteq (N \cup T)$ consists the
  non-terminal representing the top-level feature in the given feature diagram.
  The set of respective configurations is described by all words which can be
  generated by a finite number of applications of productions to the start word
  $S$.
  
  \end{itemize}
\end{definition}

Based on definition \ref{def:cgg}, we can specify an algorithm that computes the
transitive closure of the grammar by repeatedly expanding each non-terminal for
each (partial) word until word containing non-terminal symbols is left.
In addition to deriving all configurations from a grammar, the
algorithm can also be used to derive partial configurations, such as
binary-only configurations. To do so, the numeric features need to be removed
from the set of non-terminal symbols. The only limitation of this algorithm is
that, conceptually, it requires all numeric options to be mandatory features.
This is due to the unspecified semantics of a numeric option being un- or
deselected.

%\begin{algorithm}[H]\label{alg:expand}
% \KwData{Configuration generation grammar $G = \lbrace N, T, S, P \rbrace$, and
% cross-tree constraints $C$ of the given variability model}
% \KwResult{All configurations of the given variability model.}
% \vspace{5mm}
% words = Queue()\;
% words.enqueue($S$)\;
% \While{words is not empty}{
%  current = words.dequeue()\;
%  \If{current does violate any cross-tree constraint}{
%   continue\;
%   }
%   \For{$n \in N$}{
%   		\If{$n \in current$}{
%   			\For{$u \in \lbrace u | (n, u) \in P \rbrace$}{
%   				new = current\;
%   				new = new $\setminus~ n$\;
%   				new = new $\cup~ u$\;
%   				\eIf{$N \cup new = \varnothing$}{
%   					yield new\;
%   				}{
%   					words.enqueue(new)\;
%   				}
%   			}
%   			break\;
%   		}
%   }
% }
 
% \caption{Expansion algorithm for a configuration generation grammar.}
%\end{algorithm}\vspace{2mm}

\section{Configuration Sampling}\label{sec:configuration_sam}
When assessing properties for configurable software systems, it is infeasible
to consider every possible variant. As previously stated in Section 2.x, with
variability, interactions between features can emerge and can be the root cause
for configuration-related faults. Hence, faults may only be uncovered under
specific configurations. To not exhaustively assess all variants, a variety of
strategies to select a sample set of configurations have been proposed.
Every sampling strategy in the context of configurable software system is
designed with respect to a \emph{coverage criterion} 
\citep{apel_feature-oriented_2013}. For instance, most feature interactions
happen to be pair-wise interactions, i.e., interactions between two features.
Based on this finding, t-wise sampling is one widely used
strategy, where configurations are part of the sample set, so that all t-tuples
of features are selected together. Thus, all t-wise feature interactions are
covered. 

\paragraph{Binary Features.}
Most sampling strategies in the context of configurable software systems
target binary features. Popular sampling strategies for sampling configurations
of binary features include, but are not limited to the following
\citep{apel_feature-oriented_2013,medeiros_comparison_2016} strategies listed in
Table \ref{tab:sampling}.

\begin{table}[h!]
	\centering
	\begin{tabular}{lp{11cm}}
	\toprule
	\textbf{Name} & \textbf{Description} \\
	\midrule
	Feature Coverage & Configurations are selected, so that each
  feature is selected in at least one configuration
  \citep{apel_feature-oriented_2013}.\\
  \midrule
  Most-enabled-disabled & Configurations are selected, so that a
  maximal and minimal number of features is enabled
  \citep{medeiros_comparison_2016}. \\
  \midrule
  One-enabled & Configurations are selected, so
  that each feature is enabled at a time \citep{abal_42_2014}. Similarly, with a
  \emph{one-disabled} strategy, configurations are chosen, so that each feature
  is deselected at a time. \\
  \midrule
  Statement-coverage & Configurations are
  selected, so that each variable block of code (for
  compile-time variability) is at least enabled in one
  variant \citep{tartler_static_2014}. \\
  \midrule
  T-wise sampling & Configurations are selected, so that all
  t-tuples of all (binary) features are included in at least one configuration
  \citep{williams_practical_1996}. That is, the upper bound for the sample size
  is $\binom{|F|}{t}$ for features $F$ and $t \in \mathbb{N}$.\\  
	\bottomrule
	\end{tabular}
	\caption{Selection of sampling strategies for binary features.}
	\label{tab:sampling}
\end{table}

\cite{medeiros_comparison_2016} have compared different sampling strategies,
among other things with respect to resulting sample size and fault detection
rate. \emph{Most-enabled-disabled} results in the smallest
sample size, whereas \emph{t-wise} sampling, especially for a greater $t$ yields
the largest samples. Regarding the detection of faults,
\emph{statement-coverage} performed poorly, whereas \emph{t-wise} sampled
samples, especially with a greater $t$ unveiled most faults. 

\paragraph{Numeric Features.}
Similar to selecting meaningful sample sets of binary options, for numeric
options, sampling strategies are designed with respect to covering possible
interactions. Subsumed under the term \emph{design of experiments} various
sampling strategies, or experimental plans have been proposed, each assigning values
(from an option’s domain) to independent variables (numeric options, in this
case) \citep{antony_design_2014}. As the choice of an experimental plan (for
both binary and numeric options) determines the number of measurements, and therefore the cost of
assessment, not all designs might scale and be suitable for our assessments.
\cite{siegmund_performance-influence_2015} have reviewed and evaluated a number
of established experimental plans with respect to feasibility. The authors exclude a number of
designs due to an infeasible number of measurements, and advocate the use of
four designs, including the Plackett-Burman Design and Random Designs. Assuming
a discrete domain of values for each numeric option, the former design requires
each combination of levels for each pair of numeric options to occur equally.
The latter design is advocated not least because of an negligible number of
constraints among numeric options \citep{siegmund_performance-influence_2015}.
For further reading on more detailed descriptions of experimental designs, refer
to \cite{antony_design_2014}. 

Depending on whether binary, numeric, or both
types of configurations options are present, sampling strategies are selected,
respectively. To derive mixed configurations, first, samples are selected from
both binary and numeric configuration options. Second, the final sample
of mixed configurations is computed as the cross-product of binary and numeric
partial configurations.
