\documentclass[a4paper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{enumitem}


\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}

\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{YellowOrange}
\colorlet{myurlcolor}{Aquamarine}

\hypersetup{
  linkcolor  = mylinkcolor!\myshade!black,
  citecolor  = mycitecolor!\myshade!black,
  urlcolor   = myurlcolor!\myshade!black,
  colorlinks = true,
}

\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\usepackage{cite}

\title{Configuration-Related Performance Change Pattern in Highly Configurable
Software Systems}


%\usepackage{mathptmx}% http://ctan.org/pkg/mathptmx

\date{}
%

%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
\usepackage{cite}
%\usepackage{natbib}

\begin{document}
%\maketitle

%\section*{Configuration-Related Performance Change Patterns in Configurable
% Software Systems}

\section{Introduction}
Modern software systems usually ship with a variety of options to select from
in order to tailor them to customer’s needs. Although a system’s configuration
is primarily chosen to meet functional requirements, the selection of features,
of course, has effects on non-functional properties. The selection of features
can directly influence the source code of a software system (compile-time
variability) or the execution path of a software system (dynamic or load-time
variability). Moreover, non-functional properties (e.g., performance or memory
utilization) depend on the functionality offered, the respective
implementation, program load and the resulting execution; that is, the choice of
features also indirectly shapes non-functional properties. While some effects
on non-functional properties only depend on a single feature selected, effects
can also depend on a combination of features (feature interaction).
Recent research studied 

\section{State of The Art}

\subsection{Preliminary Work and Context}
\begin{enumerate}
  \item Performance Measurement (measure-based, model-based)
  \item Performance Regression and Root Cause Detection
  \item Evolution of Configurable Systems
  \item Performance Prediction of Configurable Systems (Genetic algorithms,
  Performance Influence Models)
\end{enumerate}

\subsection{Research Motivation}
Aside from evolution of code and architecture, to what extent can
variability influence performance? Why important: requirements evolve and so
does variability, a better understanding of variability changes in the presence
of existing architecture/code may lead to best practices in how to plan
architectures (extensible, modular) or to unveil sub-optimal techniques used,
e.g., regarding cross-cutting concerns or scattering\cite{siegmund_performance-influence_2015}

\section{Research Method}
\subsection{Experiment Setup}\label{sec:experiment_setup}
				  \paragraph{Revision history.}{We mine the revision history of software
				  systems from public repositories, e.g, GitHub or Sourceforge. To keep the implementation
				  effort on a feasible level, we focus on systems configurable at load-time
				  since re-compilation for each revision times a number of
				  variants needed for building a performance influence model will likely
				  exceed the scope of this thesis.}
				  
				  \paragraph{Variability History.}{In addition to the mined revision
				  history, we require a history of changes in the variability model. We do not
				  expect this information to be explicitly documented. Therefore, we will
				  have retrieve it manually from second-hand information including commit
				  messages and release notes. If run-time parameters are used,
				  inspection of parameter validation in the source code and respective
				  changes might indicate changes in the variability model.}

				  \paragraph{Detecting variability changes}{We are primarily
				  interested in the co-variance of changes in the variability model and the
				  system performance. Therefore, we limit our search space to those 
				  revisions $R$ where variability was changed. Then, for each revision $r
				  \in R$, we compare the performance of the predecessor $pre(r)$ to $r$.
				  Consequently, we will need to build $2 \times |R|$ performance influence
				  models per system studied, whereby $|R|$ denotes the number of changes in
				  the variability model.}

				  \paragraph{Variability-related performance regression.}{We will
				  build performance influence models using our previously derived feature
				  models and SPLConqueror. As the performance influence model comprises a
				  linear combination of terms representing features and feature
				  interactions, we will measure changes in performance by comparing those
				  performance influence models term-wise.}
				
\subsection{System Corpus}
				\begin{itemize}
				  \item For performance measuring, we cannot use unit tests shipped with the
				  system since tests evolve as the system itself does. That is, we need a
				  reusable and feasible test load, e.g., sample files for compression tools
				  or sample queries for database systems.

				  \item Our choice comprises systems that are configurable at load-time
				  (cf. \ref{sec:experiment_setup}). Moreover, performance needs to be
				  feasible to measure in terms of execution time and system size.

				  \item Possible candidate systems may be \emph{gzip}\footnote{},
				  \emph{snappy}\footnote{}, \emph{SQLite}\footnote{} \ldots
				\end{itemize}
			
\section{Discussion}
\section{Related Work}
\section{Conclusion}

\bibliographystyle{apalike}
\bibliography{bibliography}{}


\end{document}